{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Webscraping, 4-9-20, CRDDS Workshop Series\n",
    "## Presented by Nickoal Eichmann-Kalwara & Phil White\n",
    "\n",
    "### The following Jupyter Notebook was the basis for the Python/Beautiful Soup demonstration.\n",
    "\n",
    "Contact Phil White with questions: [philip.white@colorado.edu](mailto:philip.white@colorado.edu)\n",
    "\n",
    "Dependencies: if you're using Python, you will need to ensure requests and bs4 are installed. csv and datetime come standard. The easiest way to install these packages is to open your command prompt and type 'pip install bs4' and 'pip install requests'\n",
    "\n",
    "#### In this example, we'll scrape the html of today's NPR Politics section. View the source of the [NPR Politics page here](https://www.npr.org/sections/politics/)\n",
    "\n",
    "### Basic workflow\n",
    "\n",
    "1. Use requests.get() to call a url, and append the .text to the command to bring in the html of the page.\n",
    "\n",
    "2. bs(source, 'lxml') tells BeautifulSoup to read/parse the html\n",
    "\n",
    "3. Identify the tags you want to extract data from. It is helpful to go to the page and use right-click inspect element to figure out where the item you want is nested in the HTML structure.\n",
    "\n",
    "4. In this example, soup.h2.text grabs all of the text from in between h2 tags.\n",
    "\n",
    "5. soup.h2.a['href'] grabs the hyperlinks within an anchor tags \n",
    "\n",
    "6. We then create variables out of each line of code, naming them 'source' (the source data), 'soup' (the list that BeautifulSoup can read), and then 'headline' (first h2 tag text) and 'link' (first link within the first h2 tag)\n",
    "\n",
    "7. Finally, we printed them both out.\n",
    "\n",
    "    '>>>'  represents the Python prompt. \n",
    "\n",
    "Type all the code you find after prompts in the next cell and execute with a shift+enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary Python libraries:\n",
    "We'll use BeautifulSoup to parse html, requests to retrieve webpages, csv to write output files, datetime to timestamp our data, and pandas to view our data table.\n",
    "\n",
    "Type:\n",
    "\n",
    "    >>> from bs4 import BeautifulSoup as bs\n",
    "    >>> import requests\n",
    "    >>> import csv\n",
    "    >>> from datetime import datetime\n",
    "    >>> import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're only grabbing the first h2 and a tag from the html doc.\n",
    "\n",
    "Start by requesting the webpage:\n",
    "\n",
    "    >>> source = requests.get('https://www.npr.org/sections/politics/').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make it a list that BeautifulSoup can interpret:\n",
    "\n",
    "    >>> soup = bs(source, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the text within the first h2 tag\n",
    "\n",
    "    >>> headline = soup.h2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the link within the first h2 tag:\n",
    "\n",
    "    >>> link = soup.h2.a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print them both:\n",
    "\n",
    "    >>> print(headline)\n",
    "    >>> print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, instead of grabbing the first h2 tag, we use 'find_all' to grab all of the h2 tags.\n",
    "\n",
    "1. find_all gets all h2 tags. In this example, we filtered by class using class_ = 'title'. This got ride of h2 tags that contained info we don't want. We made a variable out of this called 'headlines.'\n",
    "2. Then, list() was used to create an empty list. We made the list a variable called linkList\n",
    "3. Next, we create a 'for loop' to iterate over each item in the headlines list. Within the loop, it finds each h2 and grabes the titles and the links (same general structure as above)\n",
    "4. Finally, the loop uses an append command to add each link into a new list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, grab the text from each h2 tag classed as 'title':\n",
    "\n",
    "    >>> headlines = soup.find_all('h2', class_ = 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll make a list of page links that we can revisit. Start by making an empty list:\n",
    "\n",
    "    >>> linkList = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a for loop. The loop iterates over each h2 in the headlines list and plucks the title headline text and link, then adds each link to the linkList list element:\n",
    "\n",
    "    >>> for items in headlines:\n",
    "            titles = items.text\n",
    "            links = items.a['href']\n",
    "            linkList.append(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we take all of the links harvested in the previous step, and scrape data from each individual page.\n",
    "\n",
    "The 'with' command opens up a new csv. You'll need to modify the path to place the output csv into your own file directory. 'a', appends each new line created to a new row in the csv. 'as f' creates a variable out of the csv.\n",
    "\n",
    "The next line uses the csv.writer function to make a new variable that writes new rows to f, out file.\n",
    "\n",
    "writer.writerow is used to write data to the cells in a row. \"Columns\" in the row go between brackets and each row is separated by a comma. To write free text, just add single quotes around it.\n",
    "\n",
    "Finally, the loop runs through the same workflow as above: First it grabs the url, then bs4 reads it, then we take the h1 tag from each of the pages and write them to a csv. \n",
    "\n",
    "Bonus: I added in a datetime function to time stamp each headline with a collected date, and wrote that as an additional column to the output csv.\n",
    "\n",
    "Note: Because the open command uses 'a' for append, if you run this again it will just add new lines to your file. 'w' in place of 'a' will rewrite it each time.\n",
    "\n",
    "    >>> with open('C:\\\\Users\\\\phwh9568\\\\Workshops\\\\WebScrape\\\\output.csv', 'a', newline = '', encoding = 'utf-8') as f: \n",
    "        writer = csv.writer(f) \n",
    "        writer.writerow(['Article Title', 'Date Collected']) \n",
    "        for links in linkList: \n",
    "            sources = requests.get(links).text \n",
    "            soups = bs(sources, 'lxml') \n",
    "            articleTitles = soups.h1.text \n",
    "            today = str(datetime.today()) \n",
    "            writer.writerow([articleTitles, today]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is the above code with comments to help understand what each line is accomplishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\phwh9568\\\\Workshops\\\\WebScrape\\\\output.csv', 'a', newline = '', encoding = 'utf-8') as f: #opens an output csv\n",
    "    writer = csv.writer(f) #creates a writing function\n",
    "    writer.writerow(['Article Title', 'Date Collected']) #writes headers into the first row of our output csv\n",
    "    for links in linkList: #iterates through each link in the linkList created above\n",
    "        sources = requests.get(links).text #grabs the html behind each link\n",
    "        soups = bs(sources, 'lxml') #BeautifulSoup reads each one\n",
    "        articleTitles = soups.h1.text #grabs h1 text for each page\n",
    "        today = str(datetime.today()) #Creates a time stamp for each run through this iterator\n",
    "        writer.writerow([articleTitles, today]) #writes the h1 text for each page to a new row and adds a timestamp on each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus!\n",
    "\n",
    "Import your csv to a Pandas dataframe so you can take a look at it.\n",
    "\n",
    "    >>> df = pd.read_csv('C:\\\\Users\\\\phwh9568\\\\Workshops\\\\WebScrape\\\\output.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type df into the next cell and view a pretty version of your data.\n",
    "\n",
    "    >>> df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ta da!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
